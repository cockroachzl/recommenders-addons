{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-03-23T15:54:04.808808Z",
     "iopub.status.busy": "2022-03-23T15:54:04.808585Z",
     "iopub.status.idle": "2022-03-23T15:54:04.811720Z",
     "shell.execute_reply": "2022-03-23T15:54:04.811164Z",
     "shell.execute_reply.started": "2022-03-23T15:54:04.808753Z"
    },
    "id": "tuOe1ymfHZPu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Recommending movies: ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfa-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders-addons/blob/master/docs/tutorials/dynamic_embedding_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "In this tutorial, we're going to use the rating data to predict the user's rating of other movies. To achieve this goal, we will follow the following steps:\n",
    "\n",
    "1. Get our data and do some preprocessing to get the required format.\n",
    "2. Implement a neural collaborative filtering(NeuralCF) model.\n",
    "3. Train the model.\n",
    "\n",
    "Different from the general recommendation model, the model we implemented replaces `tf.nn.embedding_lookup` with `tfra.dynamic_embedding.embedding_lookup`, which can handle super large sparse features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Imports\n",
    "Let's first get our imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T06:13:43.258442Z",
     "iopub.status.busy": "2022-03-25T06:13:43.258230Z",
     "iopub.status.idle": "2022-03-25T06:13:43.261137Z",
     "shell.execute_reply": "2022-03-25T06:13:43.260716Z",
     "shell.execute_reply.started": "2022-03-25T06:13:43.258387Z"
    },
    "id": "rEk-ibQkDNtF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade tensorflow-recommenders-addons\n",
    "# !pip install -q --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:54.644340Z",
     "iopub.status.busy": "2022-03-27T13:27:54.644108Z",
     "iopub.status.idle": "2022-03-27T13:27:54.650768Z",
     "shell.execute_reply": "2022-03-27T13:27:54.650207Z",
     "shell.execute_reply.started": "2022-03-27T13:27:54.644285Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:55.358104Z",
     "iopub.status.busy": "2022-03-27T13:27:55.357919Z",
     "iopub.status.idle": "2022-03-27T13:27:55.360751Z",
     "shell.execute_reply": "2022-03-27T13:27:55.360252Z",
     "shell.execute_reply.started": "2022-03-27T13:27:55.358086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:55.573121Z",
     "iopub.status.busy": "2022-03-27T13:27:55.572966Z",
     "iopub.status.idle": "2022-03-27T13:27:55.580402Z",
     "shell.execute_reply": "2022-03-27T13:27:55.580075Z",
     "shell.execute_reply.started": "2022-03-27T13:27:55.573104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:55.737018Z",
     "iopub.status.busy": "2022-03-27T13:27:55.736851Z",
     "iopub.status.idle": "2022-03-27T13:27:57.671616Z",
     "shell.execute_reply": "2022-03-27T13:27:57.671133Z",
     "shell.execute_reply.started": "2022-03-27T13:27:55.737000Z"
    },
    "id": "IqR2PQG4ZaZ0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:57.672499Z",
     "iopub.status.busy": "2022-03-27T13:27:57.672348Z",
     "iopub.status.idle": "2022-03-27T13:27:57.675306Z",
     "shell.execute_reply": "2022-03-27T13:27:57.674978Z",
     "shell.execute_reply.started": "2022-03-27T13:27:57.672482Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:57.676139Z",
     "iopub.status.busy": "2022-03-27T13:27:57.675988Z",
     "iopub.status.idle": "2022-03-27T13:27:57.728224Z",
     "shell.execute_reply": "2022-03-27T13:27:57.727827Z",
     "shell.execute_reply.started": "2022-03-27T13:27:57.676108Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# make TF not take all the GPU mem\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Single GPU\n",
    "# gpu = gpus[1]\n",
    "# tf.config.set_visible_devices(gpu, 'GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# # Multi GPUs\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(f'gpus: {gpus}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:59.142041Z",
     "iopub.status.busy": "2022-03-27T13:27:59.141860Z",
     "iopub.status.idle": "2022-03-27T13:27:59.709384Z",
     "shell.execute_reply": "2022-03-27T13:27:59.708976Z",
     "shell.execute_reply.started": "2022-03-27T13:27:59.142023Z"
    },
    "id": "ebc5d71b0b88",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:dynamic_embedding.GraphKeys has already been deprecated. The Variable will not be added to collections because it does not actully own any value, but only a holder of tables, which may lead to import_meta_graph failed since non-valued object has been added to collection. If you need to use `tf.compat.v1.train.Saver` and access all Variables from collection, you could manually add it to the collection by tf.compat.v1.add_to_collections(names, var) instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders_addons as tfra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:27:59.710224Z",
     "iopub.status.busy": "2022-03-27T13:27:59.710103Z",
     "iopub.status.idle": "2022-03-27T13:27:59.712847Z",
     "shell.execute_reply": "2022-03-27T13:27:59.712536Z",
     "shell.execute_reply.started": "2022-03-27T13:27:59.710208Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.3-dev'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfra.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:00.450253Z",
     "iopub.status.busy": "2022-03-27T13:28:00.450071Z",
     "iopub.status.idle": "2022-03-27T13:28:00.452392Z",
     "shell.execute_reply": "2022-03-27T13:28:00.451985Z",
     "shell.execute_reply.started": "2022-03-27T13:28:00.450235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SixWryS704g"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "This tutorial uses movies reviews provided by the MovieLens 100K dataset, a classic dataset from the GroupLens research group at the University of Minnesota. In order to facilitate processing, we convert the data type of `movie_id` and `user_id` into `int64`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:03.038910Z",
     "iopub.status.busy": "2022-03-27T13:28:03.038720Z",
     "iopub.status.idle": "2022-03-27T13:28:04.818335Z",
     "shell.execute_reply": "2022-03-27T13:28:04.817735Z",
     "shell.execute_reply.started": "2022-03-27T13:28:03.038892Z"
    },
    "id": "e51a15c085f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_id\": tf.strings.to_number(x[\"movie_id\"], tf.int64),\n",
    "    \"user_id\": tf.strings.to_number(x[\"user_id\"], tf.int64),\n",
    "    \"user_rating\": x[\"user_rating\"]\n",
    "})\n",
    "\n",
    "tf.random.set_seed(2021)\n",
    "shuffled = ratings.shuffle(100_000, seed=2021, reshuffle_each_iteration=False)\n",
    "\n",
    "dataset_train = shuffled.take(100_000).batch(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a79bfae6c084"
   },
   "source": [
    "## Implementing a model\n",
    "The NCFModel we implemented is very similar to the conventional one, and the main difference lies in the embedding layer. We specify the variable of embedding layer by `tfra.dynamic_embedding.get_variable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:04.916132Z",
     "iopub.status.busy": "2022-03-27T13:28:04.916001Z",
     "iopub.status.idle": "2022-03-27T13:28:04.925818Z",
     "shell.execute_reply": "2022-03-27T13:28:04.925257Z",
     "shell.execute_reply.started": "2022-03-27T13:28:04.916115Z"
    },
    "id": "a8533a80ef52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NCFModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NCFModel, self).__init__()\n",
    "        self.embedding_size = 32\n",
    "        self.d0 = Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.1),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(0.0, 0.1))\n",
    "        self.d1 = Dense(\n",
    "            64,\n",
    "            activation='relu',\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.1),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(0.0, 0.1))\n",
    "        self.d2 = Dense(\n",
    "            1,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0.0, 0.1),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(0.0, 0.1))\n",
    "        \n",
    "        self.user_embeddings = tfra.dynamic_embedding.get_variable(\n",
    "            name=\"user_dynamic_embeddings\",\n",
    "            dim=self.embedding_size,\n",
    "            initializer=tf.keras.initializers.RandomNormal(-1, 1))\n",
    "        self.user_embeddings_shadow = tfra.dynamic_embedding.shadow_ops.ShadowVariable(\n",
    "            self.user_embeddings,\n",
    "            name='user_dynamic_embeddings_shadow',\n",
    "            max_norm=None,\n",
    "            trainable=True)\n",
    "        \n",
    "        self.movie_embeddings = tfra.dynamic_embedding.get_variable(\n",
    "            name=\"moive_dynamic_embeddings\",\n",
    "            dim=self.embedding_size,\n",
    "            initializer=tf.keras.initializers.RandomNormal(-1, 1))\n",
    "        self.movie_embeddings_shadow = tfra.dynamic_embedding.shadow_ops.ShadowVariable(\n",
    "            self.movie_embeddings,\n",
    "            name='movie_dynamic_embeddings_shadow',\n",
    "            max_norm=None,\n",
    "            trainable=True)\n",
    "        \n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def call(self, batch):\n",
    "        movie_id = batch[\"movie_id\"]\n",
    "        second_movie_id = tf.stack([tf.random.shuffle(batch[\"movie_id\"]), tf.random.shuffle(batch[\"movie_id\"])], axis=1)\n",
    "        user_id = batch[\"user_id\"]\n",
    "        rating = batch[\"user_rating\"]\n",
    "\n",
    "        input_shape = tf.shape(user_id)\n",
    "        user_id_weights = tfra.dynamic_embedding.shadow_ops.embedding_lookup(self.user_embeddings_shadow, user_id)\n",
    "        user_id_weights = tf.reshape(user_id_weights, tf.concat([input_shape, [self.embedding_size]], 0))\n",
    "        \n",
    "        tf.print(user_id_weights)\n",
    "        input_shape = tf.shape(movie_id)\n",
    "        movie_id_weights = tfra.dynamic_embedding.shadow_ops.embedding_lookup(self.movie_embeddings_shadow, movie_id)\n",
    "        movie_id_weights = tf.reshape(movie_id_weights, tf.concat([input_shape, [self.embedding_size]], 0))\n",
    "        \n",
    "        input_shape = tf.shape(second_movie_id)\n",
    "        second_movie_id_weights = tfra.dynamic_embedding.shadow_ops.embedding_lookup(self.movie_embeddings_shadow, second_movie_id)\n",
    "        second_movie_id_weights = tf.reshape(second_movie_id_weights, tf.concat([input_shape, [self.embedding_size]], 0))\n",
    "        second_movie_id_weights = tfra.dynamic_embedding.keras.layers.embedding.reduce_pooling(second_movie_id_weights)\n",
    "        \n",
    "        embeddings = tf.concat([user_id_weights, movie_id_weights, second_movie_id_weights], axis=1)\n",
    "\n",
    "        dnn = self.d0(embeddings)\n",
    "        dnn = self.d1(dnn)\n",
    "        dnn = self.d2(dnn)\n",
    "        out = tf.reshape(dnn, shape=[-1])\n",
    "        loss = self.loss(rating, out)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e08009d75aa"
   },
   "source": [
    "Let's instantiate the model, and wrap the optimizer in tfra.dynamic_embedding.DynamicEmbeddingOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:06.346128Z",
     "iopub.status.busy": "2022-03-27T13:28:06.345951Z",
     "iopub.status.idle": "2022-03-27T13:28:06.482496Z",
     "shell.execute_reply": "2022-03-27T13:28:06.481872Z",
     "shell.execute_reply.started": "2022-03-27T13:28:06.346110Z"
    },
    "id": "db8405903104",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NCFModel()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer = tfra.dynamic_embedding.DynamicEmbeddingOptimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "defd4c72ed09"
   },
   "source": [
    "## Training the model\n",
    "After defining the model, we can train the model and observe the change of loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:07.853143Z",
     "iopub.status.busy": "2022-03-27T13:28:07.852955Z",
     "iopub.status.idle": "2022-03-27T13:28:07.856888Z",
     "shell.execute_reply": "2022-03-27T13:28:07.855822Z",
     "shell.execute_reply.started": "2022-03-27T13:28:07.853124Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(batch, model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(batch)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:09.631465Z",
     "iopub.status.busy": "2022-03-27T13:28:09.631283Z",
     "iopub.status.idle": "2022-03-27T13:28:09.635047Z",
     "shell.execute_reply": "2022-03-27T13:28:09.634408Z",
     "shell.execute_reply.started": "2022-03-27T13:28:09.631447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch=1):\n",
    "    for i in range(epoch):\n",
    "        total_loss = np.array([])\n",
    "        for (_, batch) in enumerate(dataset_train):\n",
    "            loss = train_step(batch, model)\n",
    "            total_loss = np.append(total_loss, loss)\n",
    "        print(\"epoch:\", i, \"mean_squared_error:\", np.mean(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T13:28:12.381120Z",
     "iopub.status.busy": "2022-03-27T13:28:12.380930Z",
     "iopub.status.idle": "2022-03-27T13:28:16.075279Z",
     "shell.execute_reply": "2022-03-27T13:28:16.074493Z",
     "shell.execute_reply.started": "2022-03-27T13:28:12.381102Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.54358864 -0.184731364 -2.0934484 ... -0.697834492 -1.60195971 -2.53917217]\n",
      " [-2.0909338 -0.0631753206 0.357673407 ... -1.14787972 -0.431725979 -1.40734792]\n",
      " [1.46237206 -1.48622155 -1.45492768 ... 0.693823814 0.0290193558 -0.166430533]\n",
      " ...\n",
      " [-0.388025165 -0.34725374 -1.80389428 ... -1.40494144 -1.46805191 -1.12663031]\n",
      " [-0.381850302 0.53756249 -0.58995831 ... -2.15224719 -1.10494184 -1.73442006]\n",
      " [-1.4699868 0.584298253 -0.219314933 ... -0.397144377 0.53237164 0.870266318]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Inputs to operation AddN of type AddN must have the same size and shape.  Input 0: [256,32] != input 1: [256,2,32] [Op:AddN]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b888f18a64ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-3eca2ca09eaa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mean_squared_error:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a91e1f8a13cf>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1082\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[0;34m(gradients)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     assert all(\n",
      "\u001b[0;32m/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_n\u001b[0;34m(inputs, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7186\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Inputs to operation AddN of type AddN must have the same size and shape.  Input 0: [256,32] != input 1: [256,2,32] [Op:AddN]"
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dynamic_embedding_tutorial.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python(3.8.8)",
   "language": "python",
   "name": "env-3.8.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
